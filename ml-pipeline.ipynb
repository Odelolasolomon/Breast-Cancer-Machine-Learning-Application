{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Collection Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    \"\"\"Load dataset from a CSV file.\"\"\"\n",
    "    data = pd.read_csv(file_path)\n",
    "    return data\n",
    "\n",
    "data = load_data(\"data/path_to_your_data.csv\")\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_numerical_features(df):\n",
    "    \"\"\"Plot histograms and box plots for numerical features.\"\"\"\n",
    "    numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    for i, feature in enumerate(numeric_features, 1):\n",
    "        plt.subplot(3, 3, i)\n",
    "        sns.histplot(df[feature], bins=30, kde=True)\n",
    "        plt.title(f\"Distribution of {feature}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    for i, feature in enumerate(numeric_features, 1):\n",
    "        plt.subplot(3, 3, i)\n",
    "        sns.boxplot(y=df[feature])\n",
    "        plt.title(f\"Boxplot of {feature}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_categorical_features(df):\n",
    "    \"\"\"Plot count plots for categorical features.\"\"\"\n",
    "    categorical_features = df.select_dtypes(include=[object]).columns.tolist()\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    for i, feature in enumerate(categorical_features, 1):\n",
    "        plt.subplot(3, 3, i)\n",
    "        sns.countplot(y=df[feature])\n",
    "        plt.title(f\"Count of {feature}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# EDA\n",
    "plot_numerical_features(data)\n",
    "plot_categorical_features(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, target_column):\n",
    "    \"\"\"Preprocess data including missing values, feature selection, and scaling.\"\"\"\n",
    "    # Identify numeric and categorical features\n",
    "    numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    categorical_features = df.select_dtypes(include=[object]).columns.tolist()\n",
    "    \n",
    "    # Remove target column from feature lists\n",
    "    numeric_features.remove(target_column)\n",
    "\n",
    "    # Handle missing values\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "    \n",
    "    # Combine preprocessing steps\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Feature Selection\n",
    "    threshold = 0.1  # Customize threshold for variance\n",
    "    low_variance_features = df.var() < threshold\n",
    "    features_to_drop = low_variance_features[low_variance_features].index.tolist()\n",
    "    \n",
    "    # Drop low variance features\n",
    "    df = df.drop(columns=features_to_drop)\n",
    "\n",
    "    # Fit and transform the data\n",
    "    X = df.drop(columns=[target_column])\n",
    "    y = df[target_column]\n",
    "    \n",
    "    X_preprocessed = preprocessor.fit_transform(X)\n",
    "\n",
    "    # Save the preprocessor\n",
    "    joblib.dump(preprocessor, '../models/preprocessor.pkl')\n",
    "\n",
    "    # Return processed data\n",
    "    return X_preprocessed, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Splitting Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y):\n",
    "    \"\"\"Split data into training, validation, and test sets.\"\"\"\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "X, y = preprocess_data(data, 'target_column')\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_data(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training and Prior Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(X_train, y_train):\n",
    "    \"\"\"Train multiple models and return the best one based on Mean Squared Error.\"\"\"\n",
    "    models = {\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'Random Forest': RandomForestRegressor(),\n",
    "        'Support Vector Regressor': SVR()\n",
    "    }\n",
    "    \n",
    "    best_model = None\n",
    "    best_score = float('inf')\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        predictions = model.predict(X_train)\n",
    "        mse = mean_squared_error(y_train, predictions)\n",
    "        \n",
    "        print(f\"{model_name} - Mean Squared Error: {mse:.2f}\")\n",
    "        \n",
    "        if mse < best_score:\n",
    "            best_score = mse\n",
    "            best_model = model\n",
    "            \n",
    "    joblib.dump(best_model, '../models/best_model.pkl')\n",
    "    return best_model\n",
    "\n",
    "best_model = train_models(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper Parameter Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_tuning(X_train, y_train):\n",
    "    \"\"\"Tune hyperparameters for the best model and return it.\"\"\"\n",
    "    param_grid = {\n",
    "        'Random Forest': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [10, 20, 30]\n",
    "        },\n",
    "        'Support Vector Regressor': {\n",
    "            'C': [0.1, 1, 10],\n",
    "            'kernel': ['linear', 'rbf']\n",
    "        }\n",
    "    }\n",
    "\n",
    "    best_model = None\n",
    "    best_score = float('inf')\n",
    "    \n",
    "    for model_name in param_grid.keys():\n",
    "        grid = GridSearchCV(models[model_name], param_grid[model_name], cv=3, scoring='neg_mean_squared_error')\n",
    "        grid.fit(X_train, y_train)\n",
    "        \n",
    "        print(f\"{model_name} - Best Parameters: {grid.best_params_}\")\n",
    "        print(f\"{model_name} - Best Score: {grid.best_score_:.2f}\")\n",
    "        \n",
    "        if grid.best_score_ < best_score:\n",
    "            best_score = grid.best_score_\n",
    "            best_model = grid.best_estimator_\n",
    "\n",
    "    joblib.dump(best_model, '../models/best_model.pkl')\n",
    "    return best_model\n",
    "\n",
    "best_model = hyperparameter_tuning(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Model Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, y, set_name=\"Validation\"):\n",
    "    \"\"\"Evaluate the model using Mean Squared Error and R² Score.\"\"\"\n",
    "    predictions = model.predict(X)\n",
    "    mse = mean_squared_error(y, predictions)\n",
    "    r2 = r2_score(y, predictions)\n",
    "    \n",
    "    print(f\"{set_name} Set Evaluation:\")\n",
    "    print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "    print(f\"R² Score: {r2:.2f}\")\n",
    "\n",
    "# Evaluate on validation and test sets\n",
    "evaluate_model(best_model, X_val, y_val, set_name=\"Validation\")\n",
    "evaluate_model(best_model, X_test, y_test, set_name=\"Test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Summary and Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_summary(model):\n",
    "    \"\"\"Save model summary to a text file.\"\"\"\n",
    "    with open('../models/model_summary.txt', 'w') as f:\n",
    "        f.write(f\"Best Model: {model.__class__.__name__}\\n\")\n",
    "        f.write(f\"Mean Squared Error: {mean_squared_error(y_val, model.predict(X_val)):.2f}\\n\")\n",
    "        f.write(f\"R² Score: {r2_score(y_val, model.predict(X_val)):.2f}\\n\")\n",
    "\n",
    "save_model_summary(best_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment Prior to Real production Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "def deploy_model(new_data_path):\n",
    "    \"\"\"Load the model and preprocessor, then make predictions on new data.\"\"\"\n",
    "    preprocessor = joblib.load('models/preprocessor.pkl')\n",
    "    model = joblib.load('models/best_model.pkl')\n",
    "    \n",
    "    # Load new data\n",
    "    new_data = pd.read_csv(new_data_path)\n",
    "    \n",
    "    # Preprocess new data\n",
    "    X_new = preprocessor.transform(new_data)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.predict(X_new)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Example of how to use the deployment function\n",
    "predictions = deploy_model('path_to_new_data.csv')\n",
    "print(predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
